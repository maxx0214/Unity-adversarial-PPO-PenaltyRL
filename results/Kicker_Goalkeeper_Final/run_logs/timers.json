{
    "name": "root",
    "gauges": {
        "Behavior_K.Policy.Entropy.mean": {
            "value": 1.524554967880249,
            "min": 1.430172324180603,
            "max": 1.5413832664489746,
            "count": 10
        },
        "Behavior_K.Policy.Entropy.sum": {
            "value": 30498.72265625,
            "min": 28554.16796875,
            "max": 30801.4609375,
            "count": 10
        },
        "Behavior_K.Environment.EpisodeLength.mean": {
            "value": 70.19217081850533,
            "min": 70.19217081850533,
            "max": 3568.6,
            "count": 10
        },
        "Behavior_K.Environment.EpisodeLength.sum": {
            "value": 19724.0,
            "min": 14827.0,
            "max": 23950.0,
            "count": 10
        },
        "Behavior_K.Self-play.ELO.mean": {
            "value": 1198.85479389957,
            "min": 1198.3295858211216,
            "max": 1200.0154838698572,
            "count": 10
        },
        "Behavior_K.Self-play.ELO.sum": {
            "value": 336878.19708577916,
            "min": 2400.0309677397145,
            "max": 336878.19708577916,
            "count": 10
        },
        "Behavior_K.Step.mean": {
            "value": 199962.0,
            "min": 19789.0,
            "max": 199962.0,
            "count": 10
        },
        "Behavior_K.Step.sum": {
            "value": 199962.0,
            "min": 19789.0,
            "max": 199962.0,
            "count": 10
        },
        "Behavior_K.Policy.ExtrinsicValueEstimate.mean": {
            "value": 6.974553108215332,
            "min": 0.02680417336523533,
            "max": 6.974553108215332,
            "count": 10
        },
        "Behavior_K.Policy.ExtrinsicValueEstimate.sum": {
            "value": 1959.849365234375,
            "min": 0.5896918177604675,
            "max": 1959.849365234375,
            "count": 10
        },
        "Behavior_K.Environment.CumulativeReward.mean": {
            "value": 9.4628616448399,
            "min": -23.956629276275635,
            "max": 9.4628616448399,
            "count": 10
        },
        "Behavior_K.Environment.CumulativeReward.sum": {
            "value": 2659.064122200012,
            "min": -119.78314638137817,
            "max": 2659.064122200012,
            "count": 10
        },
        "Behavior_K.Policy.ExtrinsicReward.mean": {
            "value": 9.4628616448399,
            "min": -23.956629276275635,
            "max": 9.4628616448399,
            "count": 10
        },
        "Behavior_K.Policy.ExtrinsicReward.sum": {
            "value": 2659.064122200012,
            "min": -119.78314638137817,
            "max": 2659.064122200012,
            "count": 10
        },
        "Behavior_K.Losses.PolicyLoss.mean": {
            "value": 0.06546758481197887,
            "min": 0.06426723537309717,
            "max": 0.0752337887271712,
            "count": 10
        },
        "Behavior_K.Losses.PolicyLoss.sum": {
            "value": 0.982013772179683,
            "min": 0.6888156260644632,
            "max": 1.023870177991274,
            "count": 10
        },
        "Behavior_K.Losses.ValueLoss.mean": {
            "value": 0.040840833239360816,
            "min": 0.021090584451374034,
            "max": 0.2146451151241652,
            "count": 10
        },
        "Behavior_K.Losses.ValueLoss.sum": {
            "value": 0.6126124985904122,
            "min": 0.21090584451374034,
            "max": 3.219676726862478,
            "count": 10
        },
        "Behavior_K.Policy.LearningRate.mean": {
            "value": 0.00024303687898771329,
            "min": 0.00024303687898771329,
            "max": 0.00029677068107643995,
            "count": 10
        },
        "Behavior_K.Policy.LearningRate.sum": {
            "value": 0.0036455531848156994,
            "min": 0.0027920766693077994,
            "max": 0.0040066852644383,
            "count": 10
        },
        "Behavior_K.Policy.Epsilon.mean": {
            "value": 0.18101228666666666,
            "min": 0.18101228666666666,
            "max": 0.19892356000000005,
            "count": 10
        },
        "Behavior_K.Policy.Epsilon.sum": {
            "value": 2.7151842999999998,
            "min": 1.9306922,
            "max": 2.8355616999999995,
            "count": 10
        },
        "Behavior_K.Policy.Beta.mean": {
            "value": 0.016204356104666667,
            "min": 0.016204356104666667,
            "max": 0.019784819644,
            "count": 10
        },
        "Behavior_K.Policy.Beta.sum": {
            "value": 0.24306534157,
            "min": 0.18614537077999999,
            "max": 0.26712878382999994,
            "count": 10
        },
        "Behavior_K.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 10
        },
        "Behavior_K.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 10
        },
        "Behavior_G.Policy.Entropy.mean": {
            "value": 1.4040058851242065,
            "min": 1.3987258672714233,
            "max": 1.4358333349227905,
            "count": 10
        },
        "Behavior_G.Policy.Entropy.sum": {
            "value": 28087.138671875,
            "min": 27949.33984375,
            "max": 29732.75390625,
            "count": 10
        },
        "Behavior_G.Environment.EpisodeLength.mean": {
            "value": 70.19217081850533,
            "min": 70.19217081850533,
            "max": 3568.6,
            "count": 10
        },
        "Behavior_G.Environment.EpisodeLength.sum": {
            "value": 19724.0,
            "min": 14827.0,
            "max": 23950.0,
            "count": 10
        },
        "Behavior_G.Step.mean": {
            "value": 199962.0,
            "min": 19789.0,
            "max": 199962.0,
            "count": 10
        },
        "Behavior_G.Step.sum": {
            "value": 199962.0,
            "min": 19789.0,
            "max": 199962.0,
            "count": 10
        },
        "Behavior_G.Policy.ExtrinsicValueEstimate.mean": {
            "value": -7.1845574378967285,
            "min": -7.1845574378967285,
            "max": -0.12924867868423462,
            "count": 10
        },
        "Behavior_G.Policy.ExtrinsicValueEstimate.sum": {
            "value": -2018.860595703125,
            "min": -2018.860595703125,
            "max": -2.843471050262451,
            "count": 10
        },
        "Behavior_G.Environment.CumulativeReward.mean": {
            "value": -10.0,
            "min": -10.0,
            "max": -4.0,
            "count": 10
        },
        "Behavior_G.Environment.CumulativeReward.sum": {
            "value": -2810.0,
            "min": -2810.0,
            "max": -20.0,
            "count": 10
        },
        "Behavior_G.Policy.ExtrinsicReward.mean": {
            "value": -10.0,
            "min": -10.0,
            "max": -4.0,
            "count": 10
        },
        "Behavior_G.Policy.ExtrinsicReward.sum": {
            "value": -2810.0,
            "min": -2810.0,
            "max": -20.0,
            "count": 10
        },
        "Behavior_G.Losses.PolicyLoss.mean": {
            "value": 0.06417956293146644,
            "min": 0.06417956293146644,
            "max": 0.07046027871021242,
            "count": 10
        },
        "Behavior_G.Losses.PolicyLoss.sum": {
            "value": 0.9626934439719965,
            "min": 0.6458398872986435,
            "max": 1.0569041806531863,
            "count": 10
        },
        "Behavior_G.Losses.ValueLoss.mean": {
            "value": 0.03669364673292471,
            "min": 0.020942602422428392,
            "max": 0.22488572190073716,
            "count": 10
        },
        "Behavior_G.Losses.ValueLoss.sum": {
            "value": 0.5504047009938706,
            "min": 0.2094260242242839,
            "max": 2.712463523067224,
            "count": 10
        },
        "Behavior_G.Policy.LearningRate.mean": {
            "value": 0.00024303687898771329,
            "min": 0.00024303687898771329,
            "max": 0.00029677068107643995,
            "count": 10
        },
        "Behavior_G.Policy.LearningRate.sum": {
            "value": 0.0036455531848156994,
            "min": 0.0027920766693077994,
            "max": 0.0040066852644383,
            "count": 10
        },
        "Behavior_G.Policy.Epsilon.mean": {
            "value": 0.18101228666666666,
            "min": 0.18101228666666666,
            "max": 0.19892356000000005,
            "count": 10
        },
        "Behavior_G.Policy.Epsilon.sum": {
            "value": 2.7151842999999998,
            "min": 1.9306922,
            "max": 2.8355616999999995,
            "count": 10
        },
        "Behavior_G.Policy.Beta.mean": {
            "value": 0.004052513104666667,
            "min": 0.004052513104666667,
            "max": 0.004946285643999999,
            "count": 10
        },
        "Behavior_G.Policy.Beta.sum": {
            "value": 0.060787696569999994,
            "min": 0.04654154078,
            "max": 0.06679452883,
            "count": 10
        },
        "Behavior_G.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 10
        },
        "Behavior_G.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 10
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1765019129",
        "python_version": "3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:01:18) [MSC v.1916 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\user\\anaconda3\\envs\\ml-agents\\Scripts\\mlagents-learn Project/config/ppo/Pong_adversarial.yaml --run-id=Kicker_Goalkeeper_Final --force",
        "mlagents_version": "1.2.0.dev0",
        "mlagents_envs_version": "1.2.0.dev0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.7.1+cu118",
        "numpy_version": "1.23.5",
        "end_time_seconds": "1765020923"
    },
    "total": 1793.6102081,
    "count": 1,
    "self": 0.011245300000155112,
    "children": {
        "run_training.setup": {
            "total": 0.08557409999957599,
            "count": 1,
            "self": 0.08557409999957599
        },
        "TrainerController.start_learning": {
            "total": 1793.5133887000002,
            "count": 1,
            "self": 3.7844203996810393,
            "children": {
                "TrainerController._reset_env": {
                    "total": 6.954253400001107,
                    "count": 6,
                    "self": 6.954253400001107
                },
                "TrainerController.advance": {
                    "total": 1782.4755344003179,
                    "count": 204962,
                    "self": 4.281347800382719,
                    "children": {
                        "env_step": {
                            "total": 1620.6414692000953,
                            "count": 204962,
                            "self": 641.8086661002062,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 976.6172995998622,
                                    "count": 204962,
                                    "self": 21.083994800179426,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 955.5333047996828,
                                            "count": 409924,
                                            "self": 955.5333047996828
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 2.215503500026898,
                                    "count": 204961,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 1721.6490984997163,
                                            "count": 204961,
                                            "is_parallel": true,
                                            "self": 1324.7862556997861,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.0016558999986955314,
                                                    "count": 12,
                                                    "is_parallel": true,
                                                    "self": 0.0008617000012236531,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.0007941999974718783,
                                                            "count": 24,
                                                            "is_parallel": true,
                                                            "self": 0.0007941999974718783
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 396.86118689993145,
                                                    "count": 204961,
                                                    "is_parallel": true,
                                                    "self": 12.817727500318142,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 11.518134299947633,
                                                            "count": 204961,
                                                            "is_parallel": true,
                                                            "self": 11.518134299947633
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 336.60873279999214,
                                                            "count": 204961,
                                                            "is_parallel": true,
                                                            "self": 336.60873279999214
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 35.91659229967354,
                                                            "count": 409922,
                                                            "is_parallel": true,
                                                            "self": 19.117111699217276,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 16.799480600456263,
                                                                    "count": 819844,
                                                                    "is_parallel": true,
                                                                    "self": 16.799480600456263
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 157.55271739983982,
                            "count": 409922,
                            "self": 10.28403229966898,
                            "children": {
                                "process_trajectory": {
                                    "total": 25.7505081001691,
                                    "count": 409922,
                                    "self": 25.7505081001691
                                },
                                "_update_policy": {
                                    "total": 121.51817700000174,
                                    "count": 256,
                                    "self": 46.18060269996113,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 75.33757430004061,
                                            "count": 9228,
                                            "self": 75.33757430004061
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "TrainerController._save_models": {
                    "total": 0.29918050000014773,
                    "count": 1,
                    "self": 0.03708020000067336,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.2621002999994744,
                            "count": 2,
                            "self": 0.2621002999994744
                        }
                    }
                }
            }
        }
    }
}